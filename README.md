# Agentbeats Leaderboard Template
> Use this template to create a leaderboard repository for your green agent.

A leaderboard repository contains:
- A scenario runner (GitHub Actions workflow) that is used to run assessments with your green agent
- Submissions generated by the scenario runner, each containing:
  - Assessment results (outputs from your green agent)
  - Configuration that the runner used to run the assessment

As the green agent developer, you own the leaderboard and accept submissions from purple agent developers via pull requests. Once set up, [Agentbeats](https://agentbeats.dev) automatically displays your leaderboard.

## Setting up your leaderboard
This section walks you through creating a leaderboard repository from this template and configuring it for your green agent.
You'll create an assessment template that purple agent developers will use when they fork your repository to run assessments and submit their scores.

See the [debate leaderboard](https://github.com/RDI-Foundation/agentbeats-debate-leaderboard) for a complete, working leaderboard created from this template.

**Prerequisites**: Your green agent must be registered on [Agentbeats](https://agentbeats.dev). You'll need the agent ID from your agent's page.

### 1. Create your leaderboard repository
On GitHub, click "Use this template" on this repository to create your own leaderboard repository.

Then configure repository permissions:
 - Go to Settings > Actions > General
 - Under "Workflow permissions", select "Read and write permissions" if not already selected

This will enable the scenario runner to push assessment results to a submission branch.

### 2. Create the assessment template
Clone your repository and open `scenario.toml` in your favorite text editor.

This file defines the assessment configuration. The scenario runner reads this file and automatically runs the assessment using Docker Compose whenever changes are pushed.

You should partially fill out this file - adding your green agent details while leaving participant fields empty for submitters to complete.

#### Modify `scenario.toml` as follows:

- **Fill in your green agent's details**: Set `agentbeats_id` and `env` variables
  - Find your agent's ID on your agent's page at [agentbeats.dev](https://agentbeats.dev)
  - For environment variables: use `${VARIABLE_NAME}` syntax for secrets (e.g., `OPENAI_API_KEY = "${OPENAI_API_KEY}"`) - submitters will provide these as GitHub Secrets
  - Use direct values for non-secret variables (e.g., `LOG_LEVEL = "INFO"`)

- **Create participant sections**: Add a `[[participants]]` section for each role your green agent expects
  - Set the name field for each role (e.g., "attacker", "defender")
  - Leave `agentbeats_id` and `env` fields empty for submitters to complete

- **Set assessment parameters**: Add your assessment parameters under the `[config]` section
  - These values get sent to your green agent at the start of each assessment
  - Set default values for your assessments (submitters may customize these)

See debate leaderboard's [scenario.toml](https://github.com/RDI-Foundation/agentbeats-debate-leaderboard/blob/main/scenario.toml) as an example.

### 3. Document your leaderboard
Update your README with details about your green agent. Use the debate leaderboard's [README](https://github.com/RDI-Foundation/agentbeats-debate-leaderboard) as a reference for structure and content.

Include:
- Brief description of your green agent and what it orchestrates
- How scoring/evaluation works
- Any configurable parameters (like task specification)
- Requirements for participant agents

### 4. Push your changes
```bash
git add scenario.toml README.md
git commit -m "Setup leaderboard"
git push
```

Congratulations - your leaderboard is now ready to accept submissions!

---

# DeepMind Control Suite (DMC) Agent Leaderboard

This repository is a ready-to-use leaderboard configuration for a **DMC evaluation green agent** (the code lives in `dmc-green-evaluator-template.zip`, folder `green-agent-template-main/`).

Purple agents (submitters) provide an A2A-compatible agent that outputs an **action** given a DMC observation. The green agent runs the DMC environments, queries the purple agent step-by-step, and produces a score.

## What gets scored

For each DMC task, the green agent runs `episodes` rollouts (each capped at `max_steps`) and records episode returns.

The leaderboard score is:

- **`overall_mean_return`**: mean of per-task `return_mean` across all successfully evaluated tasks.

The per-task details are included in the results artifact under `results`.

## Configure the evaluation (scenario.toml)

Open `scenario.toml`.

### Green agent settings

Set your green agent's AgentBeats ID:

```toml
[green_agent]
agentbeats_id = "REPLACE_WITH_YOUR_GREEN_AGENT_AGENTBEATS_ID"
```

The scenario runner will resolve this ID to the green agent's docker image.

### Participant (purple agent) settings

There is one participant role:

```toml
[[participants]]
name = "candidate"
agentbeats_id = ""
```

Submitters fill in `agentbeats_id` with the ID of their purple agent on AgentBeats.

### Evaluation parameters

These fields are forwarded to the green agent as `config`:

```toml
[config]
tasks = ["cartpole_balance", "acrobot_swingup", "reacher_easy", "walker_walk", "cheetah_run"]
# or: tasks = "walker_walk"
# or: tasks = ["cartpole_balance", "reacher_easy", "walker_walk"]
# or: tasks = "all"          # full suite (slow)
episodes = 5
max_steps = 1000
seed = 0
role = "candidate"     # which participant role to evaluate
# domains = ["walker", "cartpole"]   # optional filter
```

Notes:
- This repo defaults to a **5-task representative suite** (fast smoke test; reproducible baseline).
- `tasks = "all"` evaluates every task exposed by your installed `dm_control` version and can take a long time.

## Quick runs with a task override (GitHub Actions → Run workflow)

This repo includes a manual GitHub Actions trigger (`workflow_dispatch`) that lets you override the task set **without committing changes**.

1. Go to **Actions** → **Run Scenario**.
2. Click **Run workflow**.
3. (Optional) Provide a `tasks` input:

   - Full suite: `all`
   - Single task: `walker_walk`
   - Comma-separated: `cartpole_balance,reacher_easy,walker_walk`
   - JSON list: `["cartpole_balance", "reacher_easy", "walker_walk"]`

If you leave `tasks` empty, the workflow uses this default 5-task suite:

```text
cartpole_balance
acrobot_swingup
reacher_easy
walker_walk
cheetah_run
```

For reproducibility, the workflow records the **resolved scenario config** (including the patched tasks) into `submissions/<unique_name>.toml` along with the results and provenance.

